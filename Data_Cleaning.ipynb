{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# web scraping\n",
    "from IPython.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# data cleaning\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links to all departments' course site\n",
    "departments = [\"mgmt\",\"oid\",\"fnce\",\"statistics\",\"accounting\",\"lgst\",\"hcmg\",\"real-estate\"]\n",
    "link = []\n",
    "for i in departments:\n",
    "    link_temp = \"https://\" + i + \".wharton.upenn.edu/programs/undergraduate/course-descriptions/\"\n",
    "    link.append(link_temp)\n",
    "    \n",
    "link.append(\"https://marketing.wharton.upenn.edu/programs/undergraduate/undergradcourse/\")\n",
    "link.append('https://bepp.wharton.upenn.edu/programs/undergraduate/course-information/course-descriptions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the links to the courses\n",
    "courses = []\n",
    "for i in link:\n",
    "    r = requests.get(i)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for j in soup.find_all(\"a\"): \n",
    "        link_small = j.get(\"href\")\n",
    "        if link_small.find(\"syllabi\") > 0 and link_small.find(\"apps\") > 0:\n",
    "            courses.append(link_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the syllabus link\n",
    "syllabus = []\n",
    "missed = []\n",
    "\n",
    "for i in courses:\n",
    "    \n",
    "    try:\n",
    "        s = requests.get(i)\n",
    "        soup = BeautifulSoup(s.text, 'html.parser')\n",
    "        for i in soup.find_all(\"a\"):\n",
    "            time.sleep(random.randint(0,4))\n",
    "            temp_syl = i.get(\"href\")\n",
    "            if temp_syl.find(\"syllabi\") > 0 and temp_syl.find(\"2\") > 0:\n",
    "                syllabus.append(\"https://apps.wharton.upenn.edu\" + temp_syl)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "        missed.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pdf syllabus link\n",
    "pdf_syllabus = []\n",
    "missed_pdf = []\n",
    "\n",
    "for i in syllabus:\n",
    "    try: \n",
    "        time.sleep(random.randint(0,4))\n",
    "        z = requests.get(i)\n",
    "        soup = BeautifulSoup(z.text, 'html.parser')\n",
    "        for i in soup.find_all(\"embed\"):\n",
    "            pdf_syllabus.append(i.get(\"src\"))\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "        missed_pdf.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write files\n",
    "\n",
    "with open('pdf_syllabus.txt', 'w') as filehandle:  \n",
    "    for listitem in pdf_syllabus:\n",
    "        filehandle.write('%s\\n' % listitem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "\n",
    "f = open('pdf_syllabus_ex_bepp.txt', 'r')\n",
    "pdf_syllabi = f.readlines()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of \\n\n",
    "\n",
    "pdf_syllabus = []\n",
    "for i in range(len(pdf_syllabi)):\n",
    "    pdf_syllabus.append(pdf_syllabi[i].strip('\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf to txt\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import requests, PyPDF2, io\n",
    "\n",
    "def pdf_to_string(url):\n",
    "    listi = []\n",
    "    response = requests.get(url)\n",
    "    with io.BytesIO(response.content) as open_pdf_file:\n",
    "        read_pdf = PyPDF2.PdfFileReader(open_pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        for page_number in range(number_of_pages):   # use xrange in Py2\n",
    "            page = read_pdf.getPage(page_number)\n",
    "            page_content = page.extractText()\n",
    "            listi.append(page_content)\n",
    "    return listi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Course Review (Ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url and API key\n",
    "TOKEN = '' # insert token\n",
    "base_url = 'http://api.penncoursereview.com/courses/'\n",
    "token = '/reviews?token=' + TOKEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format term and course name for url\n",
    "\n",
    "def format_course_url(syllabi):\n",
    "    course_list = []\n",
    "    for syllabus in syllabi:\n",
    "        term= syllabus[39:44] #term\n",
    "        dept= syllabus[45:49] #dept\n",
    "        code= syllabus[49:52] #course code\n",
    "        #section= syllabus[52:55] #section\n",
    "    \n",
    "        if term != '2019A':\n",
    "            course_list.append(term.lower() + '-' + dept + '-' + code)\n",
    "    \n",
    "    return list(set(course_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get semester, course name, instructor name, ratings\n",
    "final_rating_df = pd.DataFrame([])\n",
    "error_course = []\n",
    "\n",
    "for courses in course_list:\n",
    "\n",
    "    json_file = requests.get(base_url + courses + token)\n",
    "    \n",
    "    if json_file.status_code == 200:\n",
    "        df = pd.DataFrame()\n",
    "        rating = pd.DataFrame()\n",
    "        list_sections = json.loads(json_file.content.decode('utf-8')).get('result').get('values')\n",
    "        term = courses[:5]\n",
    "\n",
    "        for i in range(len(list_sections)):\n",
    "            df = df.append([[term, \n",
    "                             list_sections[i].get('section').get('primary_alias'),\n",
    "                             list_sections[i].get('instructor').get('name').title(),\n",
    "                             list_sections[i].get('num_reviewers'),\n",
    "                             list_sections[i].get('num_students')]])\n",
    "            rating = rating.append(pd.DataFrame(list_sections[i].get('ratings'), index=[0]))\n",
    "    \n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.rename(columns={0: 'term', 1:'course', 2:'instructor', 3:'num_reviewers', 4:'num_students'})\n",
    "\n",
    "        rating = rating.reset_index(drop=True)\n",
    "        df = pd.concat([df, rating], axis=1)\n",
    "    \n",
    "        final_rating_df = final_rating_df.append(df)\n",
    "    \n",
    "    else:\n",
    "        error_course.append(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rating_df = final_rating_df.sort_values(by=['term', 'course']).reset_index(drop=True)\n",
    "\n",
    "# we're only analyzing 2010 onwards\n",
    "rating_df_2010 = final_rating_df[final_rating_df['term'] > '2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_class_name(x):\n",
    "    return str.replace(x, \"-\",\"\")\n",
    "\n",
    "vect_name = np.vectorize(replace_class_name)\n",
    "final_rating_2010['course'] = vect_name(final_rating_2010['course'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Archives (Time and Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format of url is https://www.registrar.upenn.edu/archives/18B-Course-Timetable.pdf\n",
    "first_url = 'https://www.registrar.upenn.edu/archives/'\n",
    "second_url = '-Course-Timetable.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get course time and location data for 2010-2018A\n",
    "terms = ['10A', '10C', '11A', '11C', '12A', '12C', '13A', '13B', '13C', '14A', '14B', '14C', '15A', '15B', '15C',\n",
    "        '16A', '16B', '16C', '17A', '17B', '17C', '18A']\n",
    "time_location_df = pd.DataFrame([])\n",
    "\n",
    "for term in terms:\n",
    "    # get txt of registrar data\n",
    "    time_table = first_url + term + second_url\n",
    "    formatted_output = ''.join(pdf_to_string(time_table)).replace('\\n', '')\n",
    "    \n",
    "    # regex relevant info\n",
    "    regex_all = re.findall('(?:(?:(?:ACCT|BEPP|MKTG|OIDD|FNCE|STAT|LGST|MGMT|HCMG|REAL)\\-[0-9]{3})\\s\\s[A-Z])|' + \n",
    "                       '(?:[0-9]{3}[A-Z0-9 \\n-:]{10,40}(?:VANC|JMHH|SHDH)\\s[A-Z0-9]{3})', \n",
    "                       formatted_output)\n",
    "    \n",
    "    # clean regex-ed info\n",
    "    regex_list = []\n",
    "    for i in regex_all:\n",
    "        match = re.match(r'[A-Z]{4}-[0-9]{3}', i)\n",
    "        if match:\n",
    "            regex_list.append(match.group(0))\n",
    "        else:\n",
    "            regex_list.append(i)\n",
    "            \n",
    "    # create dictionary\n",
    "    test_dict = dict()\n",
    "    for i in regex_list:\n",
    "        if len(i) == 8:\n",
    "            test_dict[key] = times\n",
    "            times = []\n",
    "            key = i\n",
    "        else:\n",
    "            if len(i) > 35:\n",
    "                times.append(key + '-' + re.findall(r'(?:[0-9]{3}[A-Z0-9 -:)]{10,25}(?:VANC|JMHH|SHDH)\\s[A-Z0-9]{3})',i)[0])\n",
    "            else:\n",
    "                times.append(key + '-' + i)\n",
    "                \n",
    "    # create dataframe\n",
    "    flattened_list = [y for x in test_dict.values() for y in x]\n",
    "    flattened_list = [i.split() for i in flattened_list]\n",
    "    df = pd.DataFrame(flattened_list).iloc[:,0:6].rename(columns={0:'course', 1:'type', 2:'day', 3:'time', 4:'building', 5:'room'})\n",
    "    df.insert(0, 'term', '20' + term.lower())\n",
    "    \n",
    "    # append df\n",
    "    time_location_df = time_location_df.append(df)\n",
    "\n",
    "time_location_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean time_location_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove hyphens from courses\n",
    "def remove_hyphen(string):\n",
    "    return string.replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hyphens from courses\n",
    "time_location_df['course'] = time_location_df['course'].apply(remove_hyphen)\n",
    "time_location_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get AM, PM or NOON\n",
    "time_indicator = re.compile(\".*\\\\-.*[0-9]([A-Z].*)\")\n",
    "def time_seperator(x):\n",
    "    match = re.findall(time_indicator, x)\n",
    "    try:\n",
    "        return match[0]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "vect_time = np.vectorize(time_seperator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split time into start and end\n",
    "number = re.compile(\"(.*[0-9])[A-Z]*$\")\n",
    "def split_string(y):\n",
    "    try:\n",
    "        z = re.findall(number, y)[0]\n",
    "        x = z.split(\"-\")\n",
    "        return x[0], x[1]\n",
    "    except:\n",
    "        return \"\",\"\"\n",
    "\n",
    "vect_split = np.vectorize(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time indicator (AM, PM, NOON) column\n",
    "time= vect_time(time_location_df['time'])\n",
    "time_location_df[\"time_indicator\"] = time\n",
    "\n",
    "# create start time and end time column\n",
    "time1, time2 = vect_split(time_location_df['time'])\n",
    "time_location_df[\"start_time\"] = time1\n",
    "time_location_df[\"end_time\"] = time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert time to decimal\n",
    "def replace_half(x):\n",
    "    if x.find(\":\")>0:\n",
    "        minute_match = x.split(\":\")[1]\n",
    "        minute = int(minute_match)\n",
    "        frac = str(minute/60)\n",
    "        minute_fin = \":\" + str(minute_match)\n",
    "        frac = '.' + frac.split('.')[1]\n",
    "        return x.replace(minute_fin,frac)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "vect_replace = np.vectorize(replace_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time to decimal\n",
    "time_location_df['start_time'] = vect_replace(time_location_df[\"start_time\"])\n",
    "time_location_df['end_time'] = vect_replace(time_location_df[\"end_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert int to float\n",
    "def to_float(x):\n",
    "    z = float(x)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert start time to float\n",
    "start_time_float = []\n",
    "for i in time_location_df['start_time']:\n",
    "    try:\n",
    "        start_time_float.append(to_float(i))\n",
    "    except:\n",
    "        start_time_float.append(to_float(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert end time to float\n",
    "end_time_float = []\n",
    "for i in time_location_df['end_time']:\n",
    "    try:\n",
    "        end_time_float.append(to_float(i))\n",
    "    except:\n",
    "        end_time_float.append(to_float(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format existing dataframe\n",
    "time_location_df['start_time'] = start_time_float\n",
    "time_location_df['end_time'] = end_time_float\n",
    "time_location_df.loc[time_location_df[\"time_indicator\"]==\"PM\",'end_time'] = time_location_df.loc[time_location_df[\"time_indicator\"]==\"PM\",'end_time'] + 12\n",
    "time_location_df.loc[(time_location_df[\"time_indicator\"]==\"PM\") & (time_location_df[\"start_time\"]<8.0),'start_time'] = time_location_df.loc[(time_location_df[\"time_indicator\"]==\"PM\") & (time_location_df[\"start_time\"]<8.0),'start_time'] + 12 \n",
    "time_location_df[\"class_duration\"] = time_location_df[\"end_time\"] - time_location_df[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge joined with syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joined data\n",
    "joined_df = pd.read_csv('joined.csv').drop(['Unnamed: 0', 'day', 'time', 'building', 'room', 'instructor'], axis=1)\n",
    "print(len(joined_df))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import syllabus data\n",
    "syllabus = pd.read_csv('syllabus_complete.csv').drop('Unnamed: 0', axis=1)\n",
    "syllabus = syllabus.rename(columns = {'class_name':'course', 'terms':'term', '0': 'url'})\n",
    "syllabus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add hyphen to course\n",
    "def add_hyphen(course):\n",
    "    return course[:4] + '-' + course[4:7] + '-' + course[7:]\n",
    "\n",
    "add_hyphen_vect = np.vectorize(add_hyphen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hyphen to course\n",
    "syllabus['course'] = add_hyphen_vect(syllabus['course'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df to get final df\n",
    "joined_df = joined_df.merge(syllabus, how='inner', on=['course','term'])\n",
    "joined_df = joined_df.merge(time_location_df, how='inner', on=['course','term'])\n",
    "joined_df = pd.concat([joined_df,pd.get_dummies(joined_df['day'])], axis=1)\n",
    "joined_df = pd.concat([joined_df,pd.get_dummies(joined_df['building'])], axis=1)\n",
    "len(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined_df.copy().dropna(subset=['test_percent', 'race', 'gender'])\n",
    "joined[\"rCourseQualityBinned\"] = joined[\"rCourseQuality\"].astype(\"float\").astype(\"int\").astype('str')\n",
    "joined['race'] = joined['race'].astype(\"float\").astype(\"int\").astype('str')\n",
    "joined['gender'] = joined['gender'].astype(\"float\").astype(\"int\").astype('str')\n",
    "\n",
    "# one hot encode gender\n",
    "joined['gender'] = encode_gender_vect(joined['gender'])\n",
    "joined = pd.concat([joined,pd.get_dummies(joined['gender'])], axis=1)\n",
    "\n",
    "# one hot encode race\n",
    "joined['race'] = encode_race_vect(joined['race'])\n",
    "joined = pd.concat([joined,pd.get_dummies(joined['race'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# joined.to_csv('joined_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean final_rating_df because some courses used grad-level listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if course is grad-level listing\n",
    "def check_if_grad_course(course):\n",
    "    return int(re.findall('[0-9]{3}', course)[0]) > 500\n",
    "\n",
    "check_if_grad_vect = np.vectorize(check_if_grad_course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(check_if_grad_vect(final_rating_df['course'].unique()))\n",
    "final_rating_df['course'].unique()[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change grad listing to undergrad listing\n",
    "def change_to_undergrad(course):\n",
    "    if course == 'STAT-512-401':\n",
    "        return 'STAT-432-401'\n",
    "    if course == 'STAT-510-401':\n",
    "        return 'STAT-430-401'\n",
    "    if course == 'STAT-701-401':\n",
    "        return 'STAT-471-401'\n",
    "    if course == 'STAT-701-402':\n",
    "        return 'STAT-471-402'\n",
    "    else:\n",
    "        return course\n",
    "    \n",
    "change_to_undergrad_vect = np.vectorize(change_to_undergrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change grad listing to undergrad listing\n",
    "final_rating_df['course'] = change_to_undergrad_vect(final_rating_df['course'])\n",
    "final_rating_2010 = final_rating_df[final_rating_df['term'] > '2010']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pdf_syllabus).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rating_2010.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_location_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
